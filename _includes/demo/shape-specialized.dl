// In shape-specialized functions, input shapes are statically known.
// This enables more optimizations and results in better performance.
// Shape-specialized for x: <1 x 784>, w: <784 x 10>, b: <10>

// f(x, w, b) = dot(x, w) + pad(b, at: 0)
func @f: (<1 x 784 x f32>, <784 x 10 x f32>, <10 x f32>) -> <1 x 10 x f32> {
'entry(%x: <1 x 784 x f32>, %w: <784 x 10 x f32>, %b: <10 x f32>):
    %0.0 = dot %x: <1 x 784 x f32>, %w: <784 x 10 x f32>
    %0.1 = padShape %b: <10 x f32> at 0
    %0.2 = add %0.0: <1 x 10 x f32>, %0.1: <1 x 10 x f32>
    return %0.2: <1 x 10 x f32>
}

// Gradient declaration in DLVM IR: [gradient @f wrt 1, 2 seedable]
// Seedable: able to take backpropagated gradient as a seed for AD
// df(x, w, b, seed) = ( df/dw, df/db )
func @df: (<1 x 784 x f32>, <784 x 10 x f32>, <1 x 10 x f32>, <1 x 10 x f32>)
         -> (<784 x 10 x f32>, <1 x 10 x f32>) {
'entry(%x: <1 x 784 x f32>, %w: <784 x 10 x f32>, %b: <1 x 10 x f32>, %seed: <1 x 10 x f32>):
    // Backward pass: df/dw = dot(x^T, seed), df/db = squeeze(seed, at: 0)
    %0.0 = squeezeShape %seed: <1 x 10 x f32> at 0
    %0.1 = transpose %x: <1 x 784 x f32>
    %0.2 = dot %0.1: <784 x 1 x f32>, %seed: <1 x 10 x f32>
    %0.3 = literal (%0.2: <784 x 10 x f32>, %0.0: <10 x f32>): (<784 x 10 x f32>, <10 x f32>)
    return %0.3: (<784 x 10 x f32>, <10 x f32>)
}

... // @g and @dg omitted here for brevity
