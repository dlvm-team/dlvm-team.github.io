---
layout: post
title:  "Welcome"
date:   2017-08-17 20:09:49 -0400
categories: welcome
---
Deep learning software demands performance and reliability. However, many current deep learning tools and infrastructures are highly dependent on software libraries that act as a dynamic DSL and a computation graph interpreter. However, interpreting computation graphs does not yield optimal performance, due to the latency between different kernel launches, constant expressions, and unoptimizable computation in automatic differentiation based on operator overloading.

We present DLVM, a design and implementation of a compiler framework that consists of linear algebra operations, automatic differentiation, domain-specific optimizations and a code generator targeting heterogeneous parallel hardware. DLVM is designed to support the development of neural network DSLs, with both AOT and JIT compilation.

To demonstrate an end-to-end system from network network DSL (via DLVM) to parallelized execution, we demonstrate NNKit, a typed tagless-final DSL for building neural networks. NNKit is embedded in the Swift programming language and targets DLVM IR.

We argue that the DLVM system enables a form of modular, safe, and performant toolkits for deep learning.
